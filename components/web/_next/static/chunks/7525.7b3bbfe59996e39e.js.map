{"version":3,"file":"static/chunks/7525.7b3bbfe59996e39e.js","mappings":"mGAAA,SAAAA,EAAAC,CAAA,EAEA,QADAC,EAAA,GACAC,EAAA,EAAAC,EAAAH,EAAAI,MAAA,CAAoCF,EAAAC,EAAO,EAAAD,EAAAD,CAAA,CAAAD,CAAA,CAAAE,EAAA,KAC3C,OAAAD,CACA,6CACA,IAAAI,EAAAN,EAAA,CACA,OACA,SACA,OACA,UACA,QACA,SACA,WACA,OACA,QACA,SACA,QACA,SACA,UACA,SACA,YACA,WACA,SACA,OACA,MACA,WACA,OACA,QACA,OACA,KACA,UACA,QACA,UACA,KACA,SACA,OACA,WACA,SACA,SACA,MACA,SACA,OACA,KACA,WACA,SACA,QACA,QACA,QACA,WACA,YACA,UACA,WACA,UACA,WACA,WACA,QACA,OACA,OACA,SACA,YACA,QACA,UACA,SACA,WACA,aACA,KACA,MACA,UACA,MACA,KACA,EACAO,EAAAP,EAAA,wCAOA,SAAAQ,EAAAC,CAAA,CAAAC,CAAA,EACA,GAAAD,EAAAE,QAAA,eACA,IAPAC,EAOAC,EAAAJ,EAAAK,IAAA,SACA,KAAAD,GAAAA,KAAAA,GARAD,EAgCA,SAAAH,CAAA,CAAAC,CAAA,EAEA,IADA,IAAAG,EAAAE,EAAA,GACA,MAAAF,CAAAA,EAAAJ,EAAAK,IAAA,MACA,GAAAD,GA1BAA,GA0BA,CAAAE,EAAA,CACAL,EAAAM,QAAA,CAAAC,GAAA,GACA,MACA,EACA,CAAAF,GAAAF,KAAAA,CACA,CACA,MAhCA,QAiCA,EAzCAH,EAAAM,QAAA,CAAAE,IAAA,CAAAN,GACAA,EAOAH,EAAAC,IACIG,KAAAA,GAAAJ,EAAAU,GAAA,OACJV,EAAAW,SAAA,GACA,WACIP,KAAAA,GAAAJ,EAAAU,GAAA,MACJ,WACI,QAAAE,IAAA,CAAAR,IACJJ,EAAAa,QAAA,kBACAb,EAAAU,GAAA,WACA,YACI,eAAAE,IAAA,CAAAR,IACJJ,EAAAa,QAAA,iBACAb,EAAAU,GAAA,WACA,YACI,iBAAAE,IAAA,CAAAR,IACJJ,EAAAa,QAAA,mBACA,YAEA,KAkBO,IAAAC,EAAA,CACPC,KAAA,SACAC,WAAA,WACA,OAAYT,SAAA,CAAAR,EAAA,CACZ,EAEAkB,MAAA,SAAAjB,CAAA,CAAAC,CAAA,EACA,IAAAiB,EAAAjB,EAAAM,QAAA,CAAAN,EAAAM,QAAA,CAAAX,MAAA,IAAAI,EAAAC,GACA,GAAAiB,YAAAA,EAAA,CACA,IAAAC,EAAAnB,EAAAoB,OAAA,GACAF,EAAArB,EAAAwB,oBAAA,CAAArB,EAAAoB,OAAA,cACAtB,EAAAuB,oBAAA,CAAArB,EAAAoB,OAAA,eACA,qBAAAR,IAAA,CAAAO,GAAA,MACA,iBAAAP,IAAA,CAAAO,GAAA,SACA,iBAAAP,IAAA,CAAAO,GAAA,SACA,uBAAAP,IAAA,CAAAO,GAAA,SACA,uCAAAP,IAAA,CAAAO,GAAA,SACA,YAAAP,IAAA,CAAAO,GAAA,SACA,WACA,OACAD,CACA,EACAI,aAAA,CACAC,cAAA,CAAoBC,KAAA,KACpB,CACA","sources":["webpack://_N_E/./node_modules/@codemirror/legacy-modes/mode/eiffel.js","webpack://_N_E/<anon>"],"sourcesContent":["function wordObj(words) {\n  var o = {};\n  for (var i = 0, e = words.length; i < e; ++i) o[words[i]] = true;\n  return o;\n}\nvar keywords = wordObj([\n  'note',\n  'across',\n  'when',\n  'variant',\n  'until',\n  'unique',\n  'undefine',\n  'then',\n  'strip',\n  'select',\n  'retry',\n  'rescue',\n  'require',\n  'rename',\n  'reference',\n  'redefine',\n  'prefix',\n  'once',\n  'old',\n  'obsolete',\n  'loop',\n  'local',\n  'like',\n  'is',\n  'inspect',\n  'infix',\n  'include',\n  'if',\n  'frozen',\n  'from',\n  'external',\n  'export',\n  'ensure',\n  'end',\n  'elseif',\n  'else',\n  'do',\n  'creation',\n  'create',\n  'check',\n  'alias',\n  'agent',\n  'separate',\n  'invariant',\n  'inherit',\n  'indexing',\n  'feature',\n  'expanded',\n  'deferred',\n  'class',\n  'Void',\n  'True',\n  'Result',\n  'Precursor',\n  'False',\n  'Current',\n  'create',\n  'attached',\n  'detachable',\n  'as',\n  'and',\n  'implies',\n  'not',\n  'or'\n]);\nvar operators = wordObj([\":=\", \"and then\",\"and\", \"or\",\"<<\",\">>\"]);\n\nfunction chain(newtok, stream, state) {\n  state.tokenize.push(newtok);\n  return newtok(stream, state);\n}\n\nfunction tokenBase(stream, state) {\n  if (stream.eatSpace()) return null;\n  var ch = stream.next();\n  if (ch == '\"'||ch == \"'\") {\n    return chain(readQuoted(ch, \"string\"), stream, state);\n  } else if (ch == \"-\"&&stream.eat(\"-\")) {\n    stream.skipToEnd();\n    return \"comment\";\n  } else if (ch == \":\"&&stream.eat(\"=\")) {\n    return \"operator\";\n  } else if (/[0-9]/.test(ch)) {\n    stream.eatWhile(/[xXbBCc0-9\\.]/);\n    stream.eat(/[\\?\\!]/);\n    return \"variable\";\n  } else if (/[a-zA-Z_0-9]/.test(ch)) {\n    stream.eatWhile(/[a-zA-Z_0-9]/);\n    stream.eat(/[\\?\\!]/);\n    return \"variable\";\n  } else if (/[=+\\-\\/*^%<>~]/.test(ch)) {\n    stream.eatWhile(/[=+\\-\\/*^%<>~]/);\n    return \"operator\";\n  } else {\n    return null;\n  }\n}\n\nfunction readQuoted(quote, style,  unescaped) {\n  return function(stream, state) {\n    var escaped = false, ch;\n    while ((ch = stream.next()) != null) {\n      if (ch == quote && (unescaped || !escaped)) {\n        state.tokenize.pop();\n        break;\n      }\n      escaped = !escaped && ch == \"%\";\n    }\n    return style;\n  };\n}\n\nexport const eiffel = {\n  name: \"eiffel\",\n  startState: function() {\n    return {tokenize: [tokenBase]};\n  },\n\n  token: function(stream, state) {\n    var style = state.tokenize[state.tokenize.length-1](stream, state);\n    if (style == \"variable\") {\n      var word = stream.current();\n      style = keywords.propertyIsEnumerable(stream.current()) ? \"keyword\"\n        : operators.propertyIsEnumerable(stream.current()) ? \"operator\"\n        : /^[A-Z][A-Z_0-9]*$/g.test(word) ? \"tag\"\n        : /^0[bB][0-1]+$/g.test(word) ? \"number\"\n        : /^0[cC][0-7]+$/g.test(word) ? \"number\"\n        : /^0[xX][a-fA-F0-9]+$/g.test(word) ? \"number\"\n        : /^([0-9]+\\.[0-9]*)|([0-9]*\\.[0-9]+)$/g.test(word) ? \"number\"\n        : /^[0-9]+$/g.test(word) ? \"number\"\n        : \"variable\";\n    }\n    return style;\n  },\n  languageData: {\n    commentTokens: {line: \"--\"}\n  }\n};\n\n"],"names":["wordObj","words","o","i","e","length","keywords","operators","tokenBase","stream","state","eatSpace","newtok","ch","next","escaped","tokenize","pop","push","eat","skipToEnd","test","eatWhile","eiffel","name","startState","token","style","word","current","propertyIsEnumerable","languageData","commentTokens","line"],"sourceRoot":""}